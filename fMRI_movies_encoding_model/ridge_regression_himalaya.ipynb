{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import check_cv\n",
    "from voxelwise_tutorials.io import load_hdf5_array\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.kernel_ridge import MultipleKernelRidgeCV, Kernelizer, ColumnKernelizer\n",
    "from himalaya.scoring import r2_score_split\n",
    "\n",
    "# Set backend\n",
    "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
    "\n",
    "# Set up paths and feature names\n",
    "directory = '/Users/mariazimmermann/dropbox/encoding_model'\n",
    "feature_names = [\"AROUSAL\", \"VALENCE\", \"SOC\", \"TOM\", \"NO\", \"FACES\"]\n",
    "\n",
    "# Load and concatenate all features\n",
    "Xs_train, Xs_test, n_features_list = [], [], []\n",
    "for feature in feature_names:\n",
    "    path = os.path.join(directory, \"features_normalized\", f\"{feature}.hdf5\")\n",
    "    Xs_train.append(load_hdf5_array(path, key=\"x_train\").astype(\"float32\"))\n",
    "    Xs_test.append(load_hdf5_array(path, key=\"x_test\").astype(\"float32\"))\n",
    "    n_features_list.append(Xs_train[-1].shape[1])\n",
    "\n",
    "X_train = np.concatenate(Xs_train, axis=1)\n",
    "X_test = np.concatenate(Xs_test, axis=1)\n",
    "\n",
    "# Define feature slices\n",
    "start_and_end = np.concatenate([[0], np.cumsum(n_features_list)])\n",
    "slices = [slice(start, end) for start, end in zip(start_and_end[:-1], start_and_end[1:])]\n",
    "\n",
    "# Run for each subject\n",
    "for sub in range(1, 22):\n",
    "    subject = f\"sub-Htriplet{sub:02d}\"\n",
    "    print(f\"\\n=== Running subject {subject} ===\")\n",
    "\n",
    "    # Load responses\n",
    "    resp_path = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
    "    Y_train = load_hdf5_array(resp_path, key=\"Y_train\")\n",
    "    Y_test = load_hdf5_array(resp_path, key=\"Y_test\")\n",
    "    run_onsets = load_hdf5_array(resp_path, key=\"run_onsets\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = check_cv(generate_leave_one_run_out(X_train.shape[0], run_onsets))\n",
    "\n",
    "    # Model configuration\n",
    "    alphas = np.logspace(1, 20, 20)\n",
    "    solver_params = dict(\n",
    "        n_iter=20,\n",
    "        alphas=alphas,\n",
    "        n_targets_batch=200,\n",
    "        n_alphas_batch=5,\n",
    "        n_targets_batch_refit=200\n",
    "    )\n",
    "\n",
    "    preprocess = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=False),\n",
    "        Delayer(delays=[1, 2, 3, 4, 5]),\n",
    "        Kernelizer(kernel=\"linear\")\n",
    "    )\n",
    "\n",
    "    kernelizers = [(name, preprocess, slc) for name, slc in zip(feature_names, slices)]\n",
    "    column_kernelizer = ColumnKernelizer(kernelizers)\n",
    "\n",
    "    model = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=\"random_search\",\n",
    "                                   solver_params=solver_params, cv=cv)\n",
    "    pipeline = make_pipeline(column_kernelizer, model)\n",
    "\n",
    "    # Fit model\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "\n",
    "    # === Full model prediction ===\n",
    "    Y_pred = backend.to_numpy(pipeline.predict(X_test))\n",
    "    Y_test_np = backend.to_numpy(Y_test)\n",
    "\n",
    "    r_vals = np.zeros(Y_test_np.shape[1])\n",
    "    r2_vals = np.zeros(Y_test_np.shape[1])\n",
    "    for v in range(Y_test_np.shape[1]):\n",
    "        r, _ = pearsonr(Y_pred[:, v], Y_test_np[:, v])\n",
    "        r_vals[v] = r\n",
    "        r2_vals[v] = r ** 2\n",
    "\n",
    "    # Save full model scores\n",
    "    np.savez(\n",
    "        os.path.join(directory, f\"full_model_scores_{subject}.npz\"),\n",
    "        r=r_vals,\n",
    "        r2=r2_vals\n",
    "    )\n",
    "\n",
    "    # === Feature-specific prediction ===\n",
    "    Y_pred_split = backend.to_numpy(pipeline.predict(X_test, split=True))  # (n_features, n_samples, n_voxels)\n",
    "\n",
    "    split_r = {}\n",
    "    split_r2 = {}\n",
    "    for i, name in enumerate(feature_names):\n",
    "        r_per_voxel = np.zeros(Y_test_np.shape[1])\n",
    "        r2_per_voxel = np.zeros(Y_test_np.shape[1])\n",
    "        for v in range(Y_test_np.shape[1]):\n",
    "            r, _ = pearsonr(Y_pred_split[i, :, v], Y_test_np[:, v])\n",
    "            r_per_voxel[v] = r\n",
    "            r2_per_voxel[v] = r ** 2\n",
    "        split_r[name] = r_per_voxel\n",
    "        split_r2[name] = r2_per_voxel\n",
    "\n",
    "    # Save per-feature r and r²\n",
    "    np.savez(os.path.join(directory, f\"split_r_{subject}.npz\"), **split_r)\n",
    "    np.savez(os.path.join(directory, f\"split_r2_{subject}.npz\"), **split_r2)\n",
    "\n",
    "    print(f\"✓ Saved: full_model_scores_{subject}.npz, split_r_*, split_r2_*\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
